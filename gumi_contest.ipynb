{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP2OmBoJPgi9u/HFIzwqwDY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahinuralam/Gumi-Industiral-Enegy-Consumption-Competition/blob/main/gumi_contest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLbx3Jlfi3R7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, Input, BatchNormalization, ReLU, Bidirectional, GRU, LeakyReLU, Activation, Concatenate\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Dataset/Gumi-dataset/gumi-1/dataset.csv')"
      ],
      "metadata": {
        "id": "Lpxu5jTXjEAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "VL_6dyCkjW6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "8B_E1dznjW9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "rzio7WdejW_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, date\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming the DataFrame is already loaded\n",
        "df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y')  # Correct format based on your data (MM/DD/YYYY)\n",
        "df.head().style.set_properties(subset=['date'], **{'background-color': 'dodgerblue'})\n"
      ],
      "metadata": {
        "id": "lmYhWvxOjXCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['company'].unique()"
      ],
      "metadata": {
        "id": "3-GFF6QnjXEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = df.groupby('company')\n"
      ],
      "metadata": {
        "id": "vhNgPTf6mEE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped"
      ],
      "metadata": {
        "id": "lPbOUarZojfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for company_name, company_data in grouped:\n",
        "    # Prepare the company-specific dataset (e.g., aggregate to daily)\n",
        "    company_data['date'] = pd.to_datetime(company_data['date'])\n",
        "    daily_consumption = company_data.groupby('date')['consumption'].sum()\n",
        "\n",
        "    # Train your model here (e.g., ARIMA, LSTM)\n",
        "    # Predict future daily consumption\n",
        "    print(f\"Predicted consumption for {company_name}:\")\n",
        "    # display predictions\n"
      ],
      "metadata": {
        "id": "nOaVZ99jokE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['date'] = pd.to_datetime(df['date'])\n",
        "daily_consumption = df.groupby('date')['consumption'].sum()\n"
      ],
      "metadata": {
        "id": "rFKVQ-1Lo6IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_consumption.head(200)"
      ],
      "metadata": {
        "id": "WB3eHJfOo8Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your dataset is already loaded into a DataFrame named 'df'\n",
        "# Iterate over unique company values\n",
        "for company in df['company'].unique():\n",
        "    # Filter the data for the current company\n",
        "    company_df = df[df['company'] == company]\n",
        "\n",
        "    # Save the filtered data to a separate CSV file\n",
        "    file_name = f\"{company}_data.csv\"\n",
        "    company_df.to_csv(file_name, index=False)\n",
        "\n",
        "    print(f\"Dataset for {company} saved as {file_name}\")\n"
      ],
      "metadata": {
        "id": "tGICaiL5o85c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cmp1 = df[df['company'] == 'Hanwha Main_data']"
      ],
      "metadata": {
        "id": "VlMXNnzArL3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cmp1 = pd.read_csv('/content/Hanwha Main_data.csv')"
      ],
      "metadata": {
        "id": "sTlS68l1rUfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cmp1"
      ],
      "metadata": {
        "id": "fZ-g_rXyrVv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cmp1.columns"
      ],
      "metadata": {
        "id": "B5rUVzwgrwwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load the dataset (Hanwha Main Dataset)\n",
        "# Assuming df is already filtered for Hanwha Main\n",
        "\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "# Convert the 'date' column to datetime\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df = df_cmp1"
      ],
      "metadata": {
        "id": "ho6-pC6AsWeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Remove rows where 'consumption' is 0\n",
        "df = df[df['consumption'] != 0]"
      ],
      "metadata": {
        "id": "q6YiTyoBvjdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 2: Group by 'date' and aggregate the 'consumption' values (you can use sum, mean, etc.)\n",
        "# We'll sum the consumption for all 24 hours to get daily consumption\n",
        "df_daily = df.groupby('date').agg({'consumption': 'sum'}).reset_index()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Step 2: Group by 'date' and aggregate the 'consumption' values (you can use sum, mean, etc.)\n",
        "# We'll sum the consumption for all 24 hours to get daily consumption\n",
        "df_daily = df.groupby('date').agg({'consumption': 'sum'}).reset_index()\n",
        "\n",
        "# Step 3: Display the new dataset with daily aggregated consumption\n",
        "print(df_daily.head())  # Displaying the first few rows of the daily aggregated data\n",
        "\n",
        "# Step 1: Remove rows where 'consumption' is 0\n",
        "df = df_daily[df_daily['consumption'] != 0]"
      ],
      "metadata": {
        "id": "q1SVy75Xvc40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you have a DataFrame 'df' with 'date' and 'hour' columns\n",
        "\n",
        "# Step 1: Convert 'hour' to string and pad with zeros if necessary (e.g., '01', '02', etc.)\n",
        "df['hour'] = df['hour'].apply(lambda x: f'{int(x):02d}')\n",
        "\n",
        "# Step 2: Concatenate 'date' and 'hour' into a single column\n",
        "df['datetime'] = df['date'] + ' ' + df['hour'] + ':00:00'  # Assuming 'hour' represents the hour part of the day\n",
        "\n",
        "\n",
        "\n",
        "# Step 4: Drop the old 'date' and 'hour' columns if they are no longer needed\n",
        "df = df.drop(columns=['date', 'hour', 'company'])\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df.head(10))\n"
      ],
      "metadata": {
        "id": "eS2Lz45t-46q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "7_cbGjyHDWnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "b52fY89M8lap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorder the columns so that 'datetime' is first and 'consumption' is second\n",
        "df = df[['datetime', 'consumption']]\n",
        "\n",
        "# Display the updated DataFrame to check the order\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "ev-GGqYgDicg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your datetime column is in string format, first convert it to datetime format\n",
        "df['datetime'] = pd.to_datetime(df['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Display the updated DataFrame with datetime as float\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "MfgQTTaUEYI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly sample 10 rows from the DataFrame\n",
        "print(df.sample(100))\n"
      ],
      "metadata": {
        "id": "tNz9nn7qZ4lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_daily = df"
      ],
      "metadata": {
        "id": "D40_SVtfDon5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the data types of each column in the DataFrame\n",
        "print(df_daily.dtypes)\n"
      ],
      "metadata": {
        "id": "sNNG3VS7E-gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming your DataFrame is called df and the 'datetime' column is already in datetime format\n",
        "df['datetime'] = pd.to_datetime(df['datetime'])  # Ensure datetime is in the correct format\n",
        "\n",
        "# Step 1: Extract year and month from the datetime column\n",
        "df['year_month'] = df['datetime'].dt.to_period('M')  # This will extract year and month (e.g., '2022-02')\n",
        "\n",
        "# Step 2: Group by the year and month and sum the consumption for each month\n",
        "monthly_consumption = df.groupby('year_month')['consumption'].sum().reset_index()\n",
        "\n",
        "# Step 4: Plot the monthly consumption\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot the total consumption per month\n",
        "plt.plot(monthly_consumption['year_month'].astype(str), monthly_consumption['consumption'], marker='o', color='blue')\n",
        "\n",
        "# Customize the plot\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Total Consumption')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RcXigxLGO58J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "P_0t3_Q1Twy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Drop the old 'date' and 'hour' columns if they are no longer needed\n",
        "df = df.drop(columns=['year_month'])"
      ],
      "metadata": {
        "id": "SXmKZPSHT-Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from pylab import rcParams\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn import metrics\n",
        "import statsmodels.tools.eval_measures as      em\n",
        "from   sklearn.metrics                 import  mean_squared_error\n",
        "from   statsmodels.tsa.api             import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
        "from   IPython.display                 import display\n",
        "from   pylab                           import rcParams"
      ],
      "metadata": {
        "id": "05aHDZS_D4Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_add_decompose = seasonal_decompose(df_daily, model = 'additive')\n",
        "df_add_decompose.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W7w71_-ADlCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Define the date range for filtering\n",
        "start_date = '2022-01-01'\n",
        "end_date = '2022-12-31'\n",
        "\n",
        "# Step 2: Filter the dataset to include only the desired date range\n",
        "df_filtered = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
        "\n",
        "# Step 3: Plot the filtered daily consumption data\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot the consumption values over time within the specified date range\n",
        "plt.plot(df_filtered['date'], df_filtered['consumption'], label='Daily Consumption', color='blue')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title(f'Daily Consumption from {start_date} to {end_date}')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Daily Consumption')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "V6kRDD0HwIos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort by date\n",
        "df = df.sort_values('datetime')\n",
        "\n",
        "# Step 3: Feature selection - we only need 'date' and 'consumption'\n",
        "data = df[['datetime', 'consumption']]\n",
        "\n",
        "# Step 4: Handle missing values (if any)\n",
        "data = data.fillna(method='ffill')  # Forward fill any missing values\n",
        "\n",
        "# Step 5: Normalize the consumption data (MinMaxScaler)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data['consumption_scaled'] = scaler.fit_transform(data[['consumption']])\n",
        "\n",
        "# Step 4: Drop the old 'date' and 'hour' columns if they are no longer needed\n",
        "data = data.drop(columns=['consumption'])"
      ],
      "metadata": {
        "id": "AuCmqB0XvmHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "3AGhYA3V_DpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(10)"
      ],
      "metadata": {
        "id": "pw4hZJG5GIrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = data"
      ],
      "metadata": {
        "id": "ZWeBWgVGUGak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet import Prophet\n",
        "import pandas as pd\n",
        "\n",
        "df_new = df[['datetime', 'consumption_scaled']].rename(columns={'datetime': 'ds', 'consumption_scaled': 'y'})\n",
        "\n",
        "# Display the result\n",
        "df_new.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "4M5UYJ7nIJba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "train_len = math.floor((df_new.shape[0]*90)/100)\n",
        "train_len"
      ],
      "metadata": {
        "id": "oQZEcJscUPs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = df_new[:train_len]\n",
        "test = df_new[train_len:]"
      ],
      "metadata": {
        "id": "CyBdn5uZUWEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.tail()\n"
      ],
      "metadata": {
        "id": "H7hYY59YUYIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()\n"
      ],
      "metadata": {
        "id": "a1CrLEA2Ua32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prophet_model = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True)\n",
        "prophet_model.add_seasonality(name='hourly', period=24, fourier_order=10)  # 24 hours in a day\n",
        "prophet_model.fit(train)"
      ],
      "metadata": {
        "id": "V7MkKgACUb22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_forecast = prophet_model.predict(test)\n",
        "test_forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].head()"
      ],
      "metadata": {
        "id": "Ac65-9LuVAgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(1)\n",
        "f.set_figheight(5)\n",
        "f.set_figwidth(15)\n",
        "fig = prophet_model.plot(test_forecast,ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VmTOtmSRVEVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prophet_model.plot_components(test_forecast)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0ybhR1l6VIN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(1)\n",
        "f.set_figheight(5)\n",
        "f.set_figwidth(15)\n",
        "ax.scatter(test.ds, test['y'], color='r')\n",
        "fig = prophet_model.plot(test_forecast, ax=ax)\n"
      ],
      "metadata": {
        "id": "n6cd6O9GVL7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "future = prophet_model.make_future_dataframe(periods=500, freq='D')\n",
        "forecast_future = prophet_model.predict(future)\n",
        "forecast_future[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].head()"
      ],
      "metadata": {
        "id": "-S8VQ61YVVkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_future[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n"
      ],
      "metadata": {
        "id": "uoCTrp4fVbqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prophet_model.plot_components(forecast_future)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VK0FwGJnVf9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(1)\n",
        "f.set_figheight(5)\n",
        "f.set_figwidth(20)\n",
        "fig = prophet_model.plot(forecast_future,ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6dM9ZpZFVkLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Merge the actual values with the forecast for comparison\n",
        "df_merged = pd.merge(df_prophet, forecast[['ds', 'yhat']], on='ds')\n",
        "\n",
        "# Step 7: Display the actual vs predicted values\n",
        "print(df_merged[['ds', 'y', 'yhat']].tail())  # Show the last few rows of actual vs predicted\n",
        "\n",
        "# Step 8: Plot the actual vs predicted values\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df_merged['ds'], df_merged['y'], label='Actual Consumption', color='blue')\n",
        "plt.plot(df_merged['ds'], df_merged['yhat'], label='Predicted Consumption', color='orange')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Consumption')\n",
        "plt.title('Actual vs Predicted Consumption')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HO67XNRwIpyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(figsize=(14,5))\n",
        "f.set_figheight(5)\n",
        "f.set_figwidth(15)\n",
        "test.plot(kind='line',x='ds', y='y', color='gray', label='Test', ax=ax)\n",
        "test_forecast.plot(kind='line',x='ds',y='yhat', color='green',label='Forecast', ax=ax)\n",
        "plt.title('Forecast vs Actuals')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6TB6GdQPVoW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def MAPE(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "\n",
        "    # Filter out zero values from y_true\n",
        "    non_zero_indices = y_true != 0\n",
        "    y_true_non_zero = y_true[non_zero_indices]\n",
        "    y_pred_non_zero = y_pred[non_zero_indices]\n",
        "\n",
        "    # Calculate MAPE only on non-zero true values\n",
        "    return np.mean(np.abs((y_true_non_zero - y_pred_non_zero) / y_true_non_zero)) * 100\n",
        "\n",
        "# Example: Calculating MAPE with zero filtering\n",
        "mape = MAPE(test['y'], test_forecast['yhat'])\n",
        "print(\"MAPE:\", round(mape, 4))\n"
      ],
      "metadata": {
        "id": "GqouJKkwVsJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a threshold for correct prediction (e.g., 10%)\n",
        "threshold = 0.50  # 10% threshold\n",
        "\n",
        "# Step 1: Calculate the percentage difference between true and predicted values\n",
        "df_merged['percentage_error'] = np.abs(df_merged['y'] - df_merged['yhat']) / df_merged['y']\n",
        "\n",
        "# Step 2: Mark predictions as correct or incorrect based on the threshold\n",
        "df_merged['correct_prediction'] = df_merged['percentage_error'] <= threshold\n",
        "\n",
        "# Step 3: Calculate the percentage of correct predictions\n",
        "correct_predictions = df_merged['correct_prediction'].sum()\n",
        "total_predictions = len(df_merged)\n",
        "accuracy_percentage = (correct_predictions / total_predictions) * 100\n",
        "\n",
        "# Step 4: Print the accuracy percentage\n",
        "print(f\"Percentage of correct predictions (within {threshold*100}% threshold): {accuracy_percentage:.2f}%\")\n",
        "\n",
        "# Show a few rows with the percentage error and correctness\n",
        "print(df_merged[['ds', 'y', 'yhat', 'percentage_error', 'correct_prediction']].head())\n"
      ],
      "metadata": {
        "id": "7oWOfbGwI_Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Define the date range for filtering\n",
        "start_date = '2022-01-01'\n",
        "end_date = '2022-02-12'  # Adjusted for the correct February date\n",
        "\n",
        "# Step 2: Filter the dataset to include only the desired date range\n",
        "df_filtered = df[(df['datetime'] >= start_date) & (df['datetime'] <= end_date)]\n",
        "\n",
        "# Step 3: Plot the filtered daily consumption data\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot the consumption values over time within the specified date range\n",
        "plt.plot(df_filtered['datetime'], df_filtered['consumption'], label='Daily Consumption', color='blue')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title(f'Daily Consumption from {start_date} to {end_date}')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Daily Consumption')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Rotate the date labels\n",
        "plt.xticks(rotation=45)  # Adjust the rotation angle if needed\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "guvUzEuH81Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "Eo2mkzbF8MNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Prepare the data for time-series forecasting\n",
        "def create_sequences(data, time_steps=60):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_steps):\n",
        "        X.append(data[i:(i + time_steps), 0])  # Append consumption values\n",
        "        y.append(data[i + time_steps, 0])      # Append the target value (next consumption)\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "I-fkeLrX8Jpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the scaled consumption data for sequence creation\n",
        "time_steps = 30  # Use the past 60 steps for prediction\n",
        "X, y = create_sequences(data['consumption_scaled'].values.reshape(-1, 1), time_steps)"
      ],
      "metadata": {
        "id": "b0ecVYFdvbQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape X to fit CNN input format (samples, time_steps, features)\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "\n",
        "# Step 7: Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
      ],
      "metadata": {
        "id": "REPaV8wEQKD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "4vcy3FIncdsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Build the improved CNN-BiLSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# CNN layers for feature extraction\n",
        "model.add(Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "model.add(BatchNormalization())  # Batch normalization layer for better convergence\n",
        "model.add(Dropout(0.3))  # Increase dropout to prevent overfitting\n",
        "\n",
        "# Additional CNN layer for deeper feature extraction\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model.add(BatchNormalization())  # Batch normalization after each Conv1D layer\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# BiLSTM layers for temporal sequence processing\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))  # First BiLSTM layer\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Second BiLSTM layer (more depth)\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Dense layers for final output\n",
        "model.add(Dense(64, activation='relu'))  # Additional dense layer for deeper learning\n",
        "model.add(Dense(1))  # Output layer (1 for regression task)\n",
        "\n",
        "# Compile the model with Adam optimizer and mean squared error loss\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Summary of the model to check layers\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "CG7cYWtp7qlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, LSTM, Bidirectional, Dropout, Dense, BatchNormalization\n",
        "\n",
        "# Build the improved CNN-BiLSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# First CNN Block with Conv1D layers\n",
        "model.add(Conv1D(filters=256, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Second CNN Block\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# No Flatten layer here! We retain the 3D shape for the LSTM\n",
        "\n",
        "# First BiLSTM layer for temporal processing\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Second BiLSTM layer\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Third BiLSTM layer\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Dense layers for final output\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Output layer (regression task)\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Summary of the model to check layers\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "fLYU7WNzyQhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Step 9: Train the model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Step 10: Make predictions\n",
        "y_pred = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "UowRQOHRvRu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse transform the predictions back to original scale\n",
        "y_pred_rescaled = scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
        "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Step 11: Plot the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(data['datetime'][-len(y_test_rescaled):], y_test_rescaled, label='True Consumption')\n",
        "plt.plot(data['datetime'][-len(y_test_rescaled):], y_pred_rescaled, label='Predicted Consumption')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Consumption')\n",
        "plt.title('True vs Predicted Consumption')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EWQnZyykBtdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define how many data points from the end of the dataset you'd like to plot (e.g., last 100 data points)\n",
        "last_n = 20  # You can adjust this value based on your desired range\n",
        "\n",
        "# Step 11: Plot the results for the last period\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plotting the true consumption for the last 'n' points\n",
        "plt.plot(data['date'][-len(y_test_rescaled):][-last_n:], y_test_rescaled[-last_n:], label='True Consumption')\n",
        "\n",
        "# Plotting the predicted consumption for the last 'n' points\n",
        "plt.plot(data['date'][-len(y_test_rescaled):][-last_n:], y_pred_rescaled[-last_n:], label='Predicted Consumption')\n",
        "\n",
        "# Customize the plot\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Consumption')\n",
        "plt.title(f'True vs Predicted Consumption (Last {last_n} Data Points)')\n",
        "plt.legend()\n",
        "\n",
        "# Rotate the date labels\n",
        "plt.xticks(rotation=45)  # Adjust the rotation angle if needed\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JaUB0nOUvRxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if there are zero or very small values in y_test_rescaled\n",
        "print(\"Zero values in y_test_rescaled:\", np.sum(y_test_rescaled == 0))\n",
        "print(\"Min value in y_test_rescaled:\", np.min(y_test_rescaled))\n"
      ],
      "metadata": {
        "id": "dS6kvK1VHSFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mape(y_true, y_pred):\n",
        "    # Avoid division by zero by filtering out zero values from y_true\n",
        "    non_zero_indices = y_true != 0\n",
        "    y_true_non_zero = y_true[non_zero_indices]\n",
        "    y_pred_non_zero = y_pred[non_zero_indices]\n",
        "\n",
        "    return np.mean(np.abs((y_true_non_zero - y_pred_non_zero) / y_true_non_zero)) * 100  # MAPE in %\n",
        "\n",
        "# Recalculate MAPE and accuracy after handling zero values\n",
        "mape = calculate_mape(y_test_rescaled, y_pred_rescaled)\n",
        "accuracy = calculate_accuracy(y_test_rescaled, y_pred_rescaled)\n",
        "\n",
        "# Print the corrected results\n",
        "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
        "print(f\"Prediction Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "9FCdNfpLHX8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare true and predicted values\n",
        "print(\"True values:\", y_test_rescaled[:10])\n",
        "print(\"Predicted values:\", y_pred_rescaled[:10])\n"
      ],
      "metadata": {
        "id": "aGxIosZKHbJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Calculate MAPE (Mean Absolute Percentage Error)\n",
        "def calculate_mape(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # MAPE in %\n",
        "\n",
        "# Step 2: Calculate the accuracy as 100% - MAPE\n",
        "def calculate_accuracy(y_true, y_pred):\n",
        "    mape = calculate_mape(y_true, y_pred)\n",
        "    accuracy = 100 - mape  # Prediction accuracy as percentage\n",
        "    return accuracy\n",
        "\n",
        "# Apply the functions to your rescaled true and predicted values\n",
        "mape = calculate_mape(y_test_rescaled, y_pred_rescaled)\n",
        "accuracy = calculate_accuracy(y_test_rescaled, y_pred_rescaled)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
        "print(f\"Prediction Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "04uN_9Cd5ZtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate residuals (difference between true and predicted values)\n",
        "residuals = y_test_rescaled - y_pred_rescaled\n",
        "\n",
        "# Plot the residuals\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(data['date'][-len(y_test_rescaled):], residuals, label='Residuals', color='red')\n",
        "plt.axhline(y=0, color='black', linestyle='--')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Residuals (True - Predicted)')\n",
        "plt.title('Residuals of Predictions')\n",
        "plt.legend()\n",
        "# Rotate the date labels\n",
        "plt.xticks(rotation=45)  # Adjust the rotation angle if needed\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "c0QS0IJTvRzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define how many data points from the end of the dataset you'd like to plot (e.g., last 15 data points)\n",
        "last_n = 15  # Adjust this value as needed\n",
        "\n",
        "# Step 11: Plot the results for the last 'n' points\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plotting the true consumption for the last 'n' points\n",
        "plt.plot(data['date'][-len(y_test_rescaled):][-last_n:], y_test_rescaled[-last_n:], label='True Consumption', color='blue')\n",
        "\n",
        "# Plotting the predicted consumption for the last 'n' points\n",
        "plt.plot(data['date'][-len(y_test_rescaled):][-last_n:], y_pred_rescaled[-last_n:], label='Predicted Consumption', color='orange')\n",
        "\n",
        "# Customize the plot\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Consumption')\n",
        "plt.title(f'True vs Predicted Consumption (Last {last_n} Data Points)')\n",
        "plt.legend()\n",
        "\n",
        "# Rotate the date labels for readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Display the plot\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TMO-htIm4YOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate residuals (difference between true and predicted values)\n",
        "filtered_residuals = filtered_y_true - filtered_y_pred\n",
        "\n",
        "# Step 4: Plot the residuals for the same date range\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(filtered_dates, filtered_residuals, label='Residuals', color='red')\n",
        "plt.axhline(y=0, color='black', linestyle='--')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Residuals (True - Predicted)')\n",
        "plt.title(f'Residuals from {start_date} to {end_date}')\n",
        "plt.legend()\n",
        "\n",
        "# Rotate date labels for readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Show the plot\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jSRG7I5P4GnR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}